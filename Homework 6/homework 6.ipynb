{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da5064c9ed8cd1fec0f36b714c345b0b",
     "grade": false,
     "grade_id": "cell-4d87ae78fb4caf9c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# NLP and the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1145e2ba69b3e56620106550f0984e0",
     "grade": false,
     "grade_id": "cell-8b531762670192f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Task 0 ~ 0P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d4cef5c9a404a5516183e6ce9bf2128",
     "grade": false,
     "grade_id": "cell-6eca7b6c6c224dde",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##### a) Please enter your group number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75470a4986487fdb40d73fa368335c95",
     "grade": true,
     "grade_id": "cell-cccfbf605a28a18a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a760dcd98f5ea360c504ee7fd50a4088",
     "grade": false,
     "grade_id": "cell-2ed8ca1b9e5204c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "_**Regarding types, documentation, and output:**_\n",
    "\n",
    "_We tried to make the description of the parameters as clear as possible. However, if you believe that something is missing, please reach out to us in Moodle. We provide type hints for the function parameters and return values of the functions that you have to implement._\n",
    "\n",
    "_Nevertheless, your code must use the provided method stubs and parameters. Furthermore, make sure that your code runs without errors and in a reasonable amount of time, for example by using \"Kernel/Restart & Run All\" before submitting._\n",
    "\n",
    "_Please use comments where appropriate to help the tutors understand your code. This is especially important for the more extensive exercises later on. Finally, please pay attention to how you output the results. We highly recommend using `display(df)` for displaying data frames._\n",
    "\n",
    "_**Please only modify the template in the specified markdown and code cells (e.g. YOUR CODE / ANSWER / IMPORTS HERE). If you add any extra cells, they wont be taken into account while grading!  Some cells are left blank on purpose. Please do not modify these cells, because they are used to autograde your submission. If these cells are modified, the automatic grading for your submission will fail and we might deduct points. Please do not modify the cells containing public and private tests. If you want to do your own tests, please use the code cell containing your code solution (YOUR CODE HERE).**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "NGK3p3euEpoS",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3db57ae64e28b10ec1c59ea20a580e26",
     "grade": false,
     "grade_id": "cell-c2c225bdbb8ec891",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# NLP and the Web: Home Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "vyEzoVz0ExZI",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b1ec96309f55c0524dd381732fa6d75",
     "grade": false,
     "grade_id": "cell-93dbc0a4ebbbf170",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "In this exercise, you will deepen the theoretical concept of QA from the lectures and learn how to analyze QA systems and how explainability in AI systems can be achieved. This will be based on the UKP-SQuARE platform which was presented during the lecture. Within this exercise, you will participate in the research community and increase the diversity of models on the UKP-SQuARE platform.\n",
    "\n",
    "For this task you should use [Google Colab](https://colab.research.google.com/) where you have free access to GPUs for fine-tuning your transformer model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "40yoiVXJE2bJ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef55ba641168b7883a1e7e48f0c1a310",
     "grade": false,
     "grade_id": "cell-9f6420be7d107600",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Task 1: Fine-tune Transformer - 5 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xm7S2UuNE7T2",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8891d28c60baf881344c68731749d3b",
     "grade": false,
     "grade_id": "cell-ea8fcbc957a8de82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "In this task, you will fine-tune a pre-trained transformer-based architecture on a QA dataset with PyTorch and the [Hugging Face](https://huggingface.co/) library. You will contribute to the community by deploying your model on Hugging Face’s Model Hub and the SQuARE platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QOQ7BsquE-cy",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af36eeaf7e0f93d2add6d31226ad90c1",
     "grade": false,
     "grade_id": "cell-1d462b6ae572a7a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**a) Finetune the assigned model with the respectively assigned dataset. Before you can do this you need to prepare your dataset. Therefore you have to download the data from Hugging Face and pass it through the training example script from Hugging Face: https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py** **(3p)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "deletable": false,
    "editable": false,
    "id": "OiGZMnY3dwco",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "421a9d18fda28308353e290d0e1a8bcf",
     "grade": false,
     "grade_id": "cell-9df184190e4b678a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "20e7e400-f4fc-4957-ace2-7c687e091912",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
      "/home/jan/Projects/NLP4Web/Homework 6/transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/jan/Projects/NLP4Web/Homework 6/transformers\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->transformers==4.38.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->transformers==4.38.0.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->transformers==4.38.0.dev0) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->transformers==4.38.0.dev0) (2024.2.2)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.38.0.dev0-py3-none-any.whl size=8457274 sha256=5f68d45daf0d48dc713b004acd0ff24480460eb910ef2f39dee71d1d04399355\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ovpkr7n6/wheels/f9/f4/a9/841ea77673ec9d3647def9a20aeac68c738993fac93722d87d\n",
      "Successfully built transformers\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.38.0.dev0\n",
      "    Uninstalling transformers-4.38.0.dev0:\n",
      "      Successfully uninstalled transformers-4.38.0.dev0\n",
      "Successfully installed transformers-4.38.0.dev0\n",
      "Requirement already satisfied: huggingface_hub in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (0.20.3)\n",
      "Requirement already satisfied: filelock in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface_hub) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->huggingface_hub) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "/home/jan/Projects/NLP4Web/Homework 6/transformers/examples/pytorch/question-answering\n",
      "Requirement already satisfied: accelerate>=0.12.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.26.1)\n",
      "Requirement already satisfied: datasets>=1.8.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.16.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (2.2.0)\n",
      "Requirement already satisfied: evaluate in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: psutil in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from accelerate>=0.12.0->-r requirements.txt (line 1)) (0.4.2)\n",
      "Requirement already satisfied: filelock in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets>=1.8.0->-r requirements.txt (line 2)) (3.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from torch>=1.3.0->-r requirements.txt (line 3)) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3.0->-r requirements.txt (line 3)) (12.3.101)\n",
      "Requirement already satisfied: responses<0.19 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from evaluate->-r requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from aiohttp->datasets>=1.8.0->-r requirements.txt (line 2)) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests>=2.19.0->datasets>=1.8.0->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from jinja2->torch>=1.3.0->-r requirements.txt (line 3)) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (2023.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from sympy->torch>=1.3.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.8.0->-r requirements.txt (line 2)) (1.16.0)\n",
      "/home/jan/Projects/NLP4Web/Homework 6\n",
      "Requirement already satisfied: huggingface-hub in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (0.20.3)\n",
      "Requirement already satisfied: filelock in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface-hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface-hub) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface-hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface-hub) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface-hub) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface-hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->huggingface-hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->huggingface-hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->huggingface-hub) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->huggingface-hub) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Download all necessary dependencies. You should not modify and only run this cell\n",
    "!git clone https://github.com/huggingface/transformers\n",
    "%cd ./transformers\n",
    "!pip install .\n",
    "!pip install huggingface_hub\n",
    "%cd ./examples/pytorch/question-answering\n",
    "!pip install -r requirements.txt\n",
    "%cd ../../../../\n",
    "!pip install huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "6Id_rdT_E6JP",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74c4aed01d4b15f928e5902d083e52a6",
     "grade": false,
     "grade_id": "cell-ce285d6eb7e25499",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "fad3e0ef-4a42-4132-c398-59b044496037",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==2.16.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: filelock in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (0.20.3)\n",
      "Requirement already satisfied: packaging in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from datasets==2.16.1) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from aiohttp->datasets==2.16.1) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets==2.16.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.16.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.16.1) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.16.1) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.16.1) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from pandas->datasets==2.16.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from pandas->datasets==2.16.1) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from pandas->datasets==2.16.1) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.16.0)\n",
      "Requirement already satisfied: tokenizers==0.15.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (0.15.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from tokenizers==0.15.0) (0.20.3)\n",
      "Requirement already satisfied: filelock in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.0) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.0) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.0) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.0) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.0) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.0) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.0) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets==2.16.1\n",
    "!pip install tokenizers==0.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0N2mYeVhPfKL",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3d3257892b17768131270e651c4acf5",
     "grade": false,
     "grade_id": "cell-f77b00213f3f9cfc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Please only run the cell to get all imports\n",
    "import json\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, AutoModel\n",
    "from typing import List, Dict,  Any\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import gzip\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rtV6xyQgPfKM",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5026fe3f9f01f9362b6c85dc7f475a65",
     "grade": false,
     "grade_id": "cell-24c64b9592f71592",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "The following code provides assigns your group a model and data set, that you should use to solve this homework. You have only to insert your group number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KvavunxFPfKM",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e845f6857c25fdf9810cd6849a9995b",
     "grade": false,
     "grade_id": "cell-1bf388bdb2d92717",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_dataset_combination(group: int) -> tuple:\n",
    "    models = ['microsoft/xtremedistil-l6-h256-uncased', 'microsoft/xtremedistil-l12-h384-uncased', 'microsoft/xtremedistil-l6-h384-uncased', 'distilbert-base-uncased','microsoft/MiniLM-L12-H384-uncased', 'huawei-noah/TinyBERT_General_4L_312D', 'huawei-noah/TinyBERT_General_6L_768D']\n",
    "    datasets = ['squad_v2']\n",
    "    result = list(itertools.product(models, datasets))\n",
    "    result = 16*result\n",
    "    # for idx, r in enumerate(result):\n",
    "    #     print(f\"Index: {idx}, Content: {r}\")\n",
    "    return result[group]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "aQPh8y9bPfKM",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d3a41536f03f065f5b2c294828887a36",
     "grade": false,
     "grade_id": "cell-67a2bf6dc03ecad5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "*In* the next subtask you should download the data set `squad_v2`.\n",
    "\n",
    "You can find more information on the Hugging Face [page](https://huggingface.co/datasets/squad_v2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "4_PsRXRDPfKN",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4d90698152d16862ce95584d6f391c9",
     "grade": false,
     "grade_id": "cell-5023740bfcaf4f0a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "c265d55e-ec6b-4c9e-cd4a-4d9380b4bf59",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: microsoft/xtremedistil-l6-h256-uncased, Dataset: squad_v2\n"
     ]
    }
   ],
   "source": [
    "# Insert your group number here and print your assigned model data set combination\n",
    "group_number = 14 # insert your group number\n",
    "model_name, dataset_name = get_model_dataset_combination(group_number)\n",
    "print(f\"Model: {model_name}, Dataset: {dataset_name}\")\n",
    "# Insert the link from the linked GitHub similar to the provided example to retrieve the data\n",
    "dataset = load_dataset('squad_v2')\n",
    "data_train = dataset['train']\n",
    "data_dev = dataset['validation']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "N8j90Dh7PfKN",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27c90a7f16fe8b7971b9e94b19a4afc5",
     "grade": false,
     "grade_id": "cell-3a05d1c268ba2e10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is how your dataset should look like:\n",
    "expected = [{\n",
    "    \"answers\": {\n",
    "        \"answer_start\": [1],\n",
    "        \"text\": [\"This is a test text\"]\n",
    "    },\n",
    "    \"context\": \"This is a test context.\",\n",
    "    \"id\": \"1\",\n",
    "    \"question\": \"Is this a test?\",\n",
    "},...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VQlydod_PfKO",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "572cd3abd0c7b71740d54bb38e2ad5db",
     "grade": true,
     "grade_id": "cell-f1a5e59e0791f430",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tests - Don't modify\n",
    "assert data_train.split == \"train\"\n",
    "assert data_dev.split == \"validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "fJqzVZgQPfKO",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ed2b73a6158590b91d6aae6d9de25a0",
     "grade": true,
     "grade_id": "cell-0dee56fbdf79a9db",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "64687bc8-8d20-4474-96c9-043c2328e174",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "{'id': '56bf6b0f3aeaaa14008c9605', 'title': 'Beyoncé', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': \"Who managed the Destiny's Child group?\", 'answers': {'text': ['Mathew Knowles'], 'answer_start': [360]}}\n",
      "{'id': '5ad39d53604f3c001a3fe8d3', 'title': 'Normans', 'context': 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.', 'question': 'Who did King Charles III swear fealty to?', 'answers': {'text': [], 'answer_start': []}}\n"
     ]
    }
   ],
   "source": [
    "# Tests - Don't modify\n",
    "import random\n",
    "idx = random.randint(0, 10)\n",
    "print(idx)\n",
    "print(data_train[idx])\n",
    "print(data_dev[idx])\n",
    "assert isinstance(data_train[idx]['context'], str)\n",
    "assert isinstance(data_train[idx]['id'], str)\n",
    "assert isinstance(data_train[idx]['question'], str)\n",
    "assert isinstance(data_dev[idx]['answers']['text'], list)\n",
    "assert isinstance(data_dev[idx]['answers']['answer_start'], list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbe058f85da5a885a39e9b32c3f8107d",
     "grade": false,
     "grade_id": "cell-d6be14507c418609",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Save the data into a JSON file and remove the questions where no answer is given.\n",
    "\n",
    "*Hint:* You might want to have a look at the `json.dumps` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "id": "D8ging4tPfKO",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cab9fd4c1d60a13aa631f1cc2dde4b86",
     "grade": false,
     "grade_id": "cell-04d882e4794a58c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_json(dataset: List[Dict[str, Any]], path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the right input format for the transformer model as json file.\n",
    "    \n",
    "    The data should be saved as a JSON. In the format of:\n",
    "        {\n",
    "        'data': [\n",
    "            {\n",
    "                \"answers\": {\n",
    "                    \"answer_start\": [1],\n",
    "                    \"text\": [\"This is a test text\"]\n",
    "                },\n",
    "                \"context\": \"This is a test context.\",\n",
    "                \"id\": \"1\",\n",
    "                \"question\": \"Is this a test?\",\n",
    "            },\n",
    "            {...},\n",
    "            ...\n",
    "        ]\n",
    "        }\n",
    "    :param dataset (List[Dict[str, Any]]): The list of input data to be saved to the file.\n",
    "    :param path (str): The path of the json file to save the data to.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    dataset_list = [entry for entry in dataset if entry.get('answers') and entry['answers'].get('text')]\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        # Use json.dump to write the data to a file, ensuring it's readable\n",
    "        json.dump({'data': dataset_list}, file, indent=2)\n",
    "\n",
    "save_json(data_train, 'train_file.json')\n",
    "save_json(data_dev, 'dev_file.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "oCSjzK_VPfKO",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2de95451a3a96d243925b9e51266427d",
     "grade": true,
     "grade_id": "cell-c6de7086aa97cc05",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tests - Don't modify\n",
    "assert [f for f in os.listdir() if re.match('train_file.json', f)]\n",
    "assert [f for f in os.listdir() if re.match('dev_file.json', f)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ieSW0zsrPfKP",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "332b8f79e434a70e74ca968042c4054c",
     "grade": false,
     "grade_id": "cell-2c95945bb0eb5604",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "b) For training your model you need to adapt the following script and execute it. This should take less than 1 hour on Google Colab with a GPU.\n",
    "\n",
    "Hint: You are working directories of Google Colab, make sure you always use the right paths and a GPU for training. **(1p)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "id": "zo1xVy5uPfKP",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8bbce0279a34a58952370dddd4003ba",
     "grade": false,
     "grade_id": "cell-d316e6b151a2f708",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define your script variables here\n",
    "MODEL_NAME = \"microsoft/xtremedistil-l6-h256-uncased\" # The one you where assigned from the function `get_model_dataset_combination`\n",
    "TRAIN_FILE_NAME = 'train_file.json' # The file name of the train file\n",
    "EVAL_FILE_NAME ='dev_file.json' # The file name of the eval file\n",
    "# # YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "nxZUcUHVPfKP",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88bc4c05d478a3b77dc99129e0fd953a",
     "grade": true,
     "grade_id": "cell-24cb3799b1b7f3e6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tests - Don't modify\n",
    "assert re.search('train_file.json', TRAIN_FILE_NAME)\n",
    "assert re.search('dev_file.json', EVAL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "LT3ux_IYPfKP",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c085a7146c304fded37cfc5d2309db2",
     "grade": false,
     "grade_id": "cell-30a36f6cbda7f076",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "c4119b3b-5156-4736-95dd-34b19becc00e",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/07/2024 16:51:20 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "02/07/2024 16:51:20 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./tmp/result/runs/Feb07_16-51-20_desktop1,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "output_dir=./tmp/result/,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=12,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./tmp/result/,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Using custom data configuration default-936c4a8b1d705cf3\n",
      "02/07/2024 16:51:21 - INFO - datasets.builder - Using custom data configuration default-936c4a8b1d705cf3\n",
      "Loading Dataset Infos from /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "02/07/2024 16:51:21 - INFO - datasets.info - Loading Dataset Infos from /home/jan/Projects/NLP4Web/nlp4web/lib/python3.11/site-packages/datasets/packaged_modules/json\n",
      "Generating dataset json (/home/jan/.cache/huggingface/datasets/json/default-936c4a8b1d705cf3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "02/07/2024 16:51:21 - INFO - datasets.builder - Generating dataset json (/home/jan/.cache/huggingface/datasets/json/default-936c4a8b1d705cf3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
      "Downloading and preparing dataset json/default to /home/jan/.cache/huggingface/datasets/json/default-936c4a8b1d705cf3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "02/07/2024 16:51:21 - INFO - datasets.builder - Downloading and preparing dataset json/default to /home/jan/.cache/huggingface/datasets/json/default-936c4a8b1d705cf3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
      "Downloading took 0.0 min\n",
      "02/07/2024 16:51:21 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "02/07/2024 16:51:21 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "02/07/2024 16:51:21 - INFO - datasets.builder - Generating train split\n",
      "Generating train split: 86821 examples [00:00, 99921.65 examples/s] \n",
      "Generating validation split\n",
      "02/07/2024 16:51:22 - INFO - datasets.builder - Generating validation split\n",
      "Generating validation split: 5928 examples [00:00, 230267.59 examples/s]\n",
      "Unable to verify splits sizes.\n",
      "02/07/2024 16:51:22 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
      "Dataset json downloaded and prepared to /home/jan/.cache/huggingface/datasets/json/default-936c4a8b1d705cf3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "02/07/2024 16:51:22 - INFO - datasets.builder - Dataset json downloaded and prepared to /home/jan/.cache/huggingface/datasets/json/default-936c4a8b1d705cf3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
      "[INFO|configuration_utils.py:729] 2024-02-07 16:51:22,313 >> loading configuration file config.json from cache at /home/jan/.cache/huggingface/hub/models--microsoft--xtremedistil-l6-h256-uncased/snapshots/8d58f0e6e83c1ab87f88d8c556ec537a111e2ee0/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-07 16:51:22,314 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.38.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:607] 2024-02-07 16:51:22,484 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:729] 2024-02-07 16:51:22,644 >> loading configuration file config.json from cache at /home/jan/.cache/huggingface/hub/models--microsoft--xtremedistil-l6-h256-uncased/snapshots/8d58f0e6e83c1ab87f88d8c556ec537a111e2ee0/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-07 16:51:22,645 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.38.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2029] 2024-02-07 16:51:23,119 >> loading file vocab.txt from cache at /home/jan/.cache/huggingface/hub/models--microsoft--xtremedistil-l6-h256-uncased/snapshots/8d58f0e6e83c1ab87f88d8c556ec537a111e2ee0/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:2029] 2024-02-07 16:51:23,119 >> loading file tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2029] 2024-02-07 16:51:23,119 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2029] 2024-02-07 16:51:23,119 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2029] 2024-02-07 16:51:23,119 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:729] 2024-02-07 16:51:23,120 >> loading configuration file config.json from cache at /home/jan/.cache/huggingface/hub/models--microsoft--xtremedistil-l6-h256-uncased/snapshots/8d58f0e6e83c1ab87f88d8c556ec537a111e2ee0/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-07 16:51:23,120 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.38.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:729] 2024-02-07 16:51:23,143 >> loading configuration file config.json from cache at /home/jan/.cache/huggingface/hub/models--microsoft--xtremedistil-l6-h256-uncased/snapshots/8d58f0e6e83c1ab87f88d8c556ec537a111e2ee0/config.json\n",
      "[INFO|configuration_utils.py:792] 2024-02-07 16:51:23,144 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"microsoft/xtremedistil-l6-h256-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.38.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3259] 2024-02-07 16:51:23,173 >> loading weights file pytorch_model.bin from cache at /home/jan/.cache/huggingface/hub/models--microsoft--xtremedistil-l6-h256-uncased/snapshots/8d58f0e6e83c1ab87f88d8c556ec537a111e2ee0/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:3982] 2024-02-07 16:51:23,191 >> Some weights of the model checkpoint at microsoft/xtremedistil-l6-h256-uncased were not used when initializing BertForQuestionAnswering: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:3994] 2024-02-07 16:51:23,191 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at microsoft/xtremedistil-l6-h256-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0%|      | 0/86821 [00:00<?, ? examples/s]Caching processed dataset at /home/jan/.cache/huggingface/datasets/json/default-936c4a8b1d705cf3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e7a98d52e6c2802.arrow\n",
      "02/07/2024 16:51:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/jan/.cache/huggingface/datasets/json/default-936c4a8b1d705cf3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9e7a98d52e6c2802.arrow\n",
      "Running tokenizer on train dataset: 100%|█| 86821/86821 [00:14<00:00, 5866.24 ex\n",
      "Running tokenizer on validation dataset:   0%|  | 0/5928 [00:00<?, ? examples/s]Caching processed dataset at /home/jan/.cache/huggingface/datasets/json/default-936c4a8b1d705cf3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-258f3136b90d657f.arrow\n",
      "02/07/2024 16:51:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /home/jan/.cache/huggingface/datasets/json/default-936c4a8b1d705cf3/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-258f3136b90d657f.arrow\n",
      "Running tokenizer on validation dataset: 100%|█| 5928/5928 [00:01<00:00, 4232.92\n",
      "[INFO|trainer.py:1747] 2024-02-07 16:51:41,675 >> ***** Running training *****\n",
      "[INFO|trainer.py:1748] 2024-02-07 16:51:41,675 >>   Num examples = 87,739\n",
      "[INFO|trainer.py:1749] 2024-02-07 16:51:41,675 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1750] 2024-02-07 16:51:41,675 >>   Instantaneous batch size per device = 12\n",
      "[INFO|trainer.py:1753] 2024-02-07 16:51:41,675 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "[INFO|trainer.py:1754] 2024-02-07 16:51:41,675 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1755] 2024-02-07 16:51:41,675 >>   Total optimization steps = 14,624\n",
      "[INFO|trainer.py:1756] 2024-02-07 16:51:41,675 >>   Number of trainable parameters = 12,684,802\n",
      "{'loss': 4.2858, 'learning_rate': 2.897428884026258e-05, 'epoch': 0.07}         \n",
      "  3%|█▎                                     | 500/14624 [00:34<16:17, 14.45it/s][INFO|trainer.py:2981] 2024-02-07 16:52:16,541 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 16:52:16,541 >> Configuration saved in ./tmp/result/tmp-checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 16:52:16,625 >> Model weights saved in ./tmp/result/tmp-checkpoint-500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 16:52:16,626 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 16:52:16,626 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-500/special_tokens_map.json\n",
      "{'loss': 2.9662, 'learning_rate': 2.7948577680525165e-05, 'epoch': 0.14}        \n",
      "  7%|██▌                                   | 1000/14624 [01:09<15:42, 14.45it/s][INFO|trainer.py:2981] 2024-02-07 16:52:51,393 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-1000\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 16:52:51,394 >> Configuration saved in ./tmp/result/tmp-checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 16:52:51,455 >> Model weights saved in ./tmp/result/tmp-checkpoint-1000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 16:52:51,455 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 16:52:51,456 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 2.3716, 'learning_rate': 2.6922866520787748e-05, 'epoch': 0.21}        \n",
      " 10%|███▉                                  | 1500/14624 [01:44<15:09, 14.43it/s][INFO|trainer.py:2981] 2024-02-07 16:53:26,204 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-1500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 16:53:26,205 >> Configuration saved in ./tmp/result/tmp-checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 16:53:26,263 >> Model weights saved in ./tmp/result/tmp-checkpoint-1500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 16:53:26,264 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 16:53:26,264 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 2.0733, 'learning_rate': 2.589715536105033e-05, 'epoch': 0.27}         \n",
      " 14%|█████▏                                | 2000/14624 [02:19<14:34, 14.44it/s][INFO|trainer.py:2981] 2024-02-07 16:54:01,027 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-2000\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 16:54:01,027 >> Configuration saved in ./tmp/result/tmp-checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 16:54:01,093 >> Model weights saved in ./tmp/result/tmp-checkpoint-2000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 16:54:01,093 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 16:54:01,093 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 1.8528, 'learning_rate': 2.4871444201312912e-05, 'epoch': 0.34}        \n",
      " 17%|██████▍                               | 2500/14624 [02:54<13:59, 14.45it/s][INFO|trainer.py:2981] 2024-02-07 16:54:35,877 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-2500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 16:54:35,877 >> Configuration saved in ./tmp/result/tmp-checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 16:54:35,943 >> Model weights saved in ./tmp/result/tmp-checkpoint-2500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 16:54:35,943 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 16:54:35,943 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 1.7323, 'learning_rate': 2.3845733041575492e-05, 'epoch': 0.41}        \n",
      " 21%|███████▊                              | 3000/14624 [03:29<13:25, 14.44it/s][INFO|trainer.py:2981] 2024-02-07 16:55:10,701 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-3000\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 16:55:10,701 >> Configuration saved in ./tmp/result/tmp-checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 16:55:10,763 >> Model weights saved in ./tmp/result/tmp-checkpoint-3000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 16:55:10,763 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 16:55:10,763 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 1.7111, 'learning_rate': 2.2820021881838072e-05, 'epoch': 0.48}        \n",
      " 24%|█████████                             | 3500/14624 [04:03<12:49, 14.45it/s][INFO|trainer.py:2981] 2024-02-07 16:55:45,512 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-3500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 16:55:45,513 >> Configuration saved in ./tmp/result/tmp-checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 16:55:45,574 >> Model weights saved in ./tmp/result/tmp-checkpoint-3500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 16:55:45,575 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 16:55:45,575 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 1.6332, 'learning_rate': 2.179431072210066e-05, 'epoch': 0.55}         \n",
      " 27%|██████████▍                           | 4000/14624 [04:38<12:15, 14.44it/s][INFO|trainer.py:2981] 2024-02-07 16:56:20,332 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-4000\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 16:56:20,333 >> Configuration saved in ./tmp/result/tmp-checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 16:56:20,398 >> Model weights saved in ./tmp/result/tmp-checkpoint-4000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 16:56:20,399 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 16:56:20,399 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 1.5844, 'learning_rate': 2.076859956236324e-05, 'epoch': 0.62}         \n",
      " 31%|███████████▋                          | 4500/14624 [05:13<11:40, 14.44it/s][INFO|trainer.py:2981] 2024-02-07 16:56:55,143 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-4500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 16:56:55,144 >> Configuration saved in ./tmp/result/tmp-checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 16:56:55,207 >> Model weights saved in ./tmp/result/tmp-checkpoint-4500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 16:56:55,207 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 16:56:55,207 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-4500/special_tokens_map.json\n",
      "{'loss': 1.5257, 'learning_rate': 1.9742888402625823e-05, 'epoch': 0.68}        \n",
      " 34%|████████████▉                         | 5000/14624 [05:48<11:06, 14.44it/s][INFO|trainer.py:2981] 2024-02-07 16:57:29,988 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-5000\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 16:57:29,989 >> Configuration saved in ./tmp/result/tmp-checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 16:57:30,057 >> Model weights saved in ./tmp/result/tmp-checkpoint-5000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 16:57:30,057 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 16:57:30,057 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 1.5217, 'learning_rate': 1.8717177242888403e-05, 'epoch': 0.75}        \n",
      " 38%|██████████████▎                       | 5500/14624 [06:23<10:32, 14.43it/s][INFO|trainer.py:2981] 2024-02-07 16:58:04,848 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-5500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 16:58:04,849 >> Configuration saved in ./tmp/result/tmp-checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 16:58:04,913 >> Model weights saved in ./tmp/result/tmp-checkpoint-5500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 16:58:04,913 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 16:58:04,913 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 1.4676, 'learning_rate': 1.7691466083150983e-05, 'epoch': 0.82}        \n",
      " 41%|███████████████▌                      | 6000/14624 [06:57<09:56, 14.45it/s][INFO|trainer.py:2981] 2024-02-07 16:58:39,588 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-6000\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 16:58:39,589 >> Configuration saved in ./tmp/result/tmp-checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 16:58:39,648 >> Model weights saved in ./tmp/result/tmp-checkpoint-6000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 16:58:39,648 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 16:58:39,648 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 1.5015, 'learning_rate': 1.6665754923413567e-05, 'epoch': 0.89}        \n",
      " 44%|████████████████▉                     | 6500/14624 [07:32<09:22, 14.45it/s][INFO|trainer.py:2981] 2024-02-07 16:59:14,366 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-6500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 16:59:14,367 >> Configuration saved in ./tmp/result/tmp-checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 16:59:14,433 >> Model weights saved in ./tmp/result/tmp-checkpoint-6500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 16:59:14,433 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 16:59:14,434 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 1.4543, 'learning_rate': 1.564004376367615e-05, 'epoch': 0.96}         \n",
      " 48%|██████████████████▏                   | 7000/14624 [08:07<08:48, 14.44it/s][INFO|trainer.py:2981] 2024-02-07 16:59:49,193 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-7000\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 16:59:49,194 >> Configuration saved in ./tmp/result/tmp-checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 16:59:49,260 >> Model weights saved in ./tmp/result/tmp-checkpoint-7000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 16:59:49,261 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 16:59:49,261 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-7000/special_tokens_map.json\n",
      "{'loss': 1.4198, 'learning_rate': 1.4614332603938733e-05, 'epoch': 1.03}        \n",
      " 51%|███████████████████▍                  | 7500/14624 [08:42<08:13, 14.44it/s][INFO|trainer.py:2981] 2024-02-07 17:00:24,016 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-7500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:00:24,017 >> Configuration saved in ./tmp/result/tmp-checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:00:24,080 >> Model weights saved in ./tmp/result/tmp-checkpoint-7500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:00:24,080 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:00:24,080 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-7500/special_tokens_map.json\n",
      "{'loss': 1.3572, 'learning_rate': 1.3588621444201313e-05, 'epoch': 1.09}        \n",
      " 55%|████████████████████▊                 | 8000/14624 [09:17<07:38, 14.45it/s][INFO|trainer.py:2981] 2024-02-07 17:00:58,822 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-8000\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:00:58,823 >> Configuration saved in ./tmp/result/tmp-checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:00:58,882 >> Model weights saved in ./tmp/result/tmp-checkpoint-8000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:00:58,882 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:00:58,882 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-8000/special_tokens_map.json\n",
      "{'loss': 1.3292, 'learning_rate': 1.2562910284463895e-05, 'epoch': 1.16}        \n",
      " 58%|██████████████████████                | 8500/14624 [09:51<07:03, 14.45it/s][INFO|trainer.py:2981] 2024-02-07 17:01:33,611 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-8500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:01:33,612 >> Configuration saved in ./tmp/result/tmp-checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:01:33,678 >> Model weights saved in ./tmp/result/tmp-checkpoint-8500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:01:33,678 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:01:33,678 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-8500/special_tokens_map.json\n",
      "{'loss': 1.3293, 'learning_rate': 1.1537199124726478e-05, 'epoch': 1.23}        \n",
      " 62%|███████████████████████▍              | 9000/14624 [10:26<06:29, 14.45it/s][INFO|trainer.py:2981] 2024-02-07 17:02:08,437 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-9000\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:02:08,438 >> Configuration saved in ./tmp/result/tmp-checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:02:08,501 >> Model weights saved in ./tmp/result/tmp-checkpoint-9000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:02:08,501 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:02:08,501 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-9000/special_tokens_map.json\n",
      "{'loss': 1.3272, 'learning_rate': 1.0511487964989058e-05, 'epoch': 1.3}         \n",
      " 65%|████████████████████████▋             | 9500/14624 [11:01<05:54, 14.44it/s][INFO|trainer.py:2981] 2024-02-07 17:02:43,245 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-9500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:02:43,246 >> Configuration saved in ./tmp/result/tmp-checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:02:43,305 >> Model weights saved in ./tmp/result/tmp-checkpoint-9500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:02:43,306 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:02:43,306 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-9500/special_tokens_map.json\n",
      "{'loss': 1.327, 'learning_rate': 9.485776805251642e-06, 'epoch': 1.37}          \n",
      " 68%|█████████████████████████▎           | 10000/14624 [11:36<05:20, 14.44it/s][INFO|trainer.py:2981] 2024-02-07 17:03:18,074 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-10000\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:03:18,074 >> Configuration saved in ./tmp/result/tmp-checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:03:18,136 >> Model weights saved in ./tmp/result/tmp-checkpoint-10000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:03:18,136 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:03:18,137 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-10000/special_tokens_map.json\n",
      "{'loss': 1.3341, 'learning_rate': 8.460065645514224e-06, 'epoch': 1.44}         \n",
      " 72%|██████████████████████████▌          | 10500/14624 [12:11<04:45, 14.44it/s][INFO|trainer.py:2981] 2024-02-07 17:03:52,878 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-10500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:03:52,878 >> Configuration saved in ./tmp/result/tmp-checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:03:52,933 >> Model weights saved in ./tmp/result/tmp-checkpoint-10500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:03:52,933 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:03:52,933 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-10500/special_tokens_map.json\n",
      "{'loss': 1.3181, 'learning_rate': 7.434354485776806e-06, 'epoch': 1.5}          \n",
      " 75%|███████████████████████████▊         | 11000/14624 [12:45<04:10, 14.49it/s][INFO|trainer.py:2981] 2024-02-07 17:04:27,612 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-11000\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:04:27,613 >> Configuration saved in ./tmp/result/tmp-checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:04:27,666 >> Model weights saved in ./tmp/result/tmp-checkpoint-11000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:04:27,666 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:04:27,666 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-11000/special_tokens_map.json\n",
      "{'loss': 1.2881, 'learning_rate': 6.408643326039388e-06, 'epoch': 1.57}         \n",
      " 79%|█████████████████████████████        | 11500/14624 [13:20<03:36, 14.44it/s][INFO|trainer.py:2981] 2024-02-07 17:05:02,406 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-11500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:05:02,406 >> Configuration saved in ./tmp/result/tmp-checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:05:02,469 >> Model weights saved in ./tmp/result/tmp-checkpoint-11500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:05:02,469 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:05:02,469 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-11500/special_tokens_map.json\n",
      "{'loss': 1.3228, 'learning_rate': 5.3829321663019695e-06, 'epoch': 1.64}        \n",
      " 82%|██████████████████████████████▎      | 12000/14624 [13:55<03:01, 14.45it/s][INFO|trainer.py:2981] 2024-02-07 17:05:37,206 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-12000\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:05:37,207 >> Configuration saved in ./tmp/result/tmp-checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:05:37,265 >> Model weights saved in ./tmp/result/tmp-checkpoint-12000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:05:37,265 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:05:37,265 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-12000/special_tokens_map.json\n",
      "{'loss': 1.321, 'learning_rate': 4.357221006564551e-06, 'epoch': 1.71}          \n",
      " 85%|███████████████████████████████▋     | 12500/14624 [14:30<02:27, 14.45it/s][INFO|trainer.py:2981] 2024-02-07 17:06:12,020 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-12500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:06:12,021 >> Configuration saved in ./tmp/result/tmp-checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:06:12,086 >> Model weights saved in ./tmp/result/tmp-checkpoint-12500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:06:12,087 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:06:12,087 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-12500/special_tokens_map.json\n",
      "{'loss': 1.294, 'learning_rate': 3.3315098468271337e-06, 'epoch': 1.78}         \n",
      " 89%|████████████████████████████████▉    | 13000/14624 [15:05<01:52, 14.44it/s][INFO|trainer.py:2981] 2024-02-07 17:06:46,845 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-13000\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:06:46,846 >> Configuration saved in ./tmp/result/tmp-checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:06:46,908 >> Model weights saved in ./tmp/result/tmp-checkpoint-13000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:06:46,908 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:06:46,908 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-13000/special_tokens_map.json\n",
      "{'loss': 1.2798, 'learning_rate': 2.3057986870897156e-06, 'epoch': 1.85}        \n",
      " 92%|██████████████████████████████████▏  | 13500/14624 [15:39<01:17, 14.44it/s][INFO|trainer.py:2981] 2024-02-07 17:07:21,646 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-13500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:07:21,646 >> Configuration saved in ./tmp/result/tmp-checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:07:21,706 >> Model weights saved in ./tmp/result/tmp-checkpoint-13500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:07:21,706 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:07:21,707 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-13500/special_tokens_map.json\n",
      "{'loss': 1.3109, 'learning_rate': 1.2800875273522977e-06, 'epoch': 1.91}        \n",
      " 96%|███████████████████████████████████▍ | 14000/14624 [16:14<00:43, 14.44it/s][INFO|trainer.py:2981] 2024-02-07 17:07:56,480 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-14000\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:07:56,480 >> Configuration saved in ./tmp/result/tmp-checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:07:56,543 >> Model weights saved in ./tmp/result/tmp-checkpoint-14000/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:07:56,543 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:07:56,543 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-14000/special_tokens_map.json\n",
      "{'loss': 1.2766, 'learning_rate': 2.543763676148797e-07, 'epoch': 1.98}         \n",
      " 99%|████████████████████████████████████▋| 14500/14624 [16:49<00:08, 14.45it/s][INFO|trainer.py:2981] 2024-02-07 17:08:31,306 >> Saving model checkpoint to ./tmp/result/tmp-checkpoint-14500\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:08:31,307 >> Configuration saved in ./tmp/result/tmp-checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:08:31,361 >> Model weights saved in ./tmp/result/tmp-checkpoint-14500/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:08:31,362 >> tokenizer config file saved in ./tmp/result/tmp-checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:08:31,362 >> Special tokens file saved in ./tmp/result/tmp-checkpoint-14500/special_tokens_map.json\n",
      "100%|████████████████████████████████████▉| 14623/14624 [16:58<00:00, 14.40it/s][INFO|trainer.py:1988] 2024-02-07 17:08:40,045 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1018.3716, 'train_samples_per_second': 172.312, 'train_steps_per_second': 14.36, 'train_loss': 1.63557712776134, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████| 14624/14624 [16:58<00:00, 14.36it/s]\n",
      "[INFO|trainer.py:2981] 2024-02-07 17:08:40,048 >> Saving model checkpoint to ./tmp/result/\n",
      "[INFO|configuration_utils.py:473] 2024-02-07 17:08:40,048 >> Configuration saved in ./tmp/result/config.json\n",
      "[INFO|modeling_utils.py:2454] 2024-02-07 17:08:40,104 >> Model weights saved in ./tmp/result/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2435] 2024-02-07 17:08:40,105 >> tokenizer config file saved in ./tmp/result/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2444] 2024-02-07 17:08:40,105 >> Special tokens file saved in ./tmp/result/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     1.6356\n",
      "  train_runtime            = 0:16:58.37\n",
      "  train_samples            =      87739\n",
      "  train_samples_per_second =    172.312\n",
      "  train_steps_per_second   =      14.36\n",
      "02/07/2024 17:08:40 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:737] 2024-02-07 17:08:40,112 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3287] 2024-02-07 17:08:40,113 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3289] 2024-02-07 17:08:40,113 >>   Num examples = 6056\n",
      "[INFO|trainer.py:3292] 2024-02-07 17:08:40,113 >>   Batch size = 8\n",
      " 99%|████████████████████████████████████████▋| 752/757 [00:13<00:00, 55.55it/s]02/07/2024 17:08:57 - INFO - utils_qa - Post-processing 5928 example predictions split into 6056 features.\n",
      "\n",
      "  0%|                                                  | 0/5928 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|▍                                       | 72/5928 [00:00<00:08, 712.19it/s]\u001b[A\n",
      "  3%|▉                                      | 149/5928 [00:00<00:07, 744.50it/s]\u001b[A\n",
      "  4%|█▍                                     | 225/5928 [00:00<00:07, 751.31it/s]\u001b[A\n",
      "  5%|█▉                                     | 302/5928 [00:00<00:07, 756.16it/s]\u001b[A\n",
      "  6%|██▍                                    | 378/5928 [00:00<00:07, 755.24it/s]\u001b[A\n",
      "  8%|███                                    | 457/5928 [00:00<00:07, 766.20it/s]\u001b[A\n",
      "  9%|███▌                                   | 534/5928 [00:00<00:07, 765.60it/s]\u001b[A\n",
      " 10%|████                                   | 611/5928 [00:00<00:06, 760.43it/s]\u001b[A\n",
      " 12%|████▌                                  | 688/5928 [00:00<00:06, 755.60it/s]\u001b[A\n",
      " 13%|█████                                  | 764/5928 [00:01<00:06, 752.05it/s]\u001b[A\n",
      " 14%|█████▌                                 | 840/5928 [00:01<00:06, 746.99it/s]\u001b[A\n",
      " 15%|██████                                 | 915/5928 [00:01<00:06, 741.17it/s]\u001b[A\n",
      " 17%|██████▌                                | 990/5928 [00:01<00:06, 742.42it/s]\u001b[A\n",
      " 18%|██████▊                               | 1065/5928 [00:01<00:06, 742.41it/s]\u001b[A\n",
      " 19%|███████▎                              | 1140/5928 [00:01<00:06, 740.87it/s]\u001b[A\n",
      " 20%|███████▊                              | 1215/5928 [00:01<00:06, 740.83it/s]\u001b[A\n",
      " 22%|████████▎                             | 1290/5928 [00:01<00:06, 738.13it/s]\u001b[A\n",
      " 23%|████████▊                             | 1365/5928 [00:01<00:06, 739.66it/s]\u001b[A\n",
      " 24%|█████████▏                            | 1441/5928 [00:01<00:06, 743.81it/s]\u001b[A\n",
      " 26%|█████████▋                            | 1516/5928 [00:02<00:05, 738.53it/s]\u001b[A\n",
      " 27%|██████████▏                           | 1590/5928 [00:02<00:06, 707.05it/s]\u001b[A\n",
      " 28%|██████████▋                           | 1661/5928 [00:02<00:06, 650.00it/s]\u001b[A\n",
      " 29%|███████████                           | 1727/5928 [00:02<00:08, 523.48it/s]\u001b[A\n",
      " 30%|███████████▌                          | 1801/5928 [00:02<00:07, 575.03it/s]\u001b[A\n",
      " 32%|████████████                          | 1876/5928 [00:02<00:06, 619.05it/s]\u001b[A\n",
      " 33%|████████████▍                         | 1946/5928 [00:02<00:06, 640.41it/s]\u001b[A\n",
      " 34%|████████████▉                         | 2014/5928 [00:02<00:06, 649.50it/s]\u001b[A\n",
      " 35%|█████████████▎                        | 2082/5928 [00:02<00:05, 652.78it/s]\u001b[A\n",
      " 36%|█████████████▊                        | 2154/5928 [00:03<00:05, 666.62it/s]\u001b[A\n",
      " 38%|██████████████▎                       | 2225/5928 [00:03<00:05, 677.26it/s]\u001b[A\n",
      " 39%|██████████████▋                       | 2294/5928 [00:03<00:05, 670.36it/s]\u001b[A\n",
      " 40%|███████████████▏                      | 2368/5928 [00:03<00:05, 688.80it/s]\u001b[A\n",
      " 41%|███████████████▋                      | 2440/5928 [00:03<00:05, 696.44it/s]\u001b[A\n",
      " 42%|████████████████                      | 2515/5928 [00:03<00:04, 709.31it/s]\u001b[A\n",
      " 44%|████████████████▌                     | 2587/5928 [00:03<00:04, 702.86it/s]\u001b[A\n",
      " 45%|█████████████████                     | 2662/5928 [00:03<00:04, 716.39it/s]\u001b[A\n",
      " 46%|█████████████████▌                    | 2737/5928 [00:03<00:04, 725.02it/s]\u001b[A\n",
      " 47%|██████████████████                    | 2810/5928 [00:03<00:04, 719.18it/s]\u001b[A\n",
      " 49%|██████████████████▍                   | 2884/5928 [00:04<00:04, 723.51it/s]\u001b[A\n",
      " 50%|██████████████████▉                   | 2959/5928 [00:04<00:04, 731.05it/s]\u001b[A\n",
      " 51%|███████████████████▍                  | 3033/5928 [00:04<00:03, 731.92it/s]\u001b[A\n",
      " 52%|███████████████████▉                  | 3108/5928 [00:04<00:03, 737.18it/s]\u001b[A\n",
      " 54%|████████████████████▍                 | 3182/5928 [00:04<00:03, 731.60it/s]\u001b[A\n",
      " 55%|████████████████████▊                 | 3256/5928 [00:04<00:03, 732.63it/s]\u001b[A\n",
      " 56%|█████████████████████▎                | 3330/5928 [00:04<00:03, 697.33it/s]\u001b[A\n",
      " 57%|█████████████████████▊                | 3403/5928 [00:04<00:03, 706.41it/s]\u001b[A\n",
      " 59%|██████████████████████▎               | 3477/5928 [00:04<00:03, 714.70it/s]\u001b[A\n",
      " 60%|██████████████████████▊               | 3551/5928 [00:05<00:03, 722.00it/s]\u001b[A\n",
      " 61%|███████████████████████▏              | 3624/5928 [00:05<00:03, 722.49it/s]\u001b[A\n",
      " 62%|███████████████████████▋              | 3697/5928 [00:05<00:03, 720.63it/s]\u001b[A\n",
      " 64%|████████████████████████▏             | 3771/5928 [00:05<00:02, 724.01it/s]\u001b[A\n",
      " 65%|████████████████████████▋             | 3844/5928 [00:05<00:02, 715.57it/s]\u001b[A\n",
      " 66%|█████████████████████████             | 3916/5928 [00:05<00:02, 695.92it/s]\u001b[A\n",
      " 67%|█████████████████████████▌            | 3990/5928 [00:05<00:02, 705.83it/s]\u001b[A\n",
      " 69%|██████████████████████████            | 4063/5928 [00:05<00:02, 709.75it/s]\u001b[A\n",
      " 70%|██████████████████████████▌           | 4137/5928 [00:05<00:02, 716.36it/s]\u001b[A\n",
      " 71%|██████████████████████████▉           | 4209/5928 [00:05<00:02, 712.27it/s]\u001b[A\n",
      " 72%|███████████████████████████▍          | 4284/5928 [00:06<00:02, 723.23it/s]\u001b[A\n",
      " 74%|███████████████████████████▉          | 4362/5928 [00:06<00:02, 738.11it/s]\u001b[A\n",
      " 75%|████████████████████████████▍         | 4440/5928 [00:06<00:01, 748.81it/s]\u001b[A\n",
      " 76%|████████████████████████████▉         | 4518/5928 [00:06<00:01, 757.27it/s]\u001b[A\n",
      " 78%|█████████████████████████████▍        | 4598/5928 [00:06<00:01, 767.13it/s]\u001b[A\n",
      " 79%|█████████████████████████████▉        | 4676/5928 [00:06<00:01, 769.54it/s]\u001b[A\n",
      " 80%|██████████████████████████████▍       | 4755/5928 [00:06<00:01, 773.30it/s]\u001b[A\n",
      " 82%|██████████████████████████████▉       | 4835/5928 [00:06<00:01, 779.40it/s]\u001b[A\n",
      " 83%|███████████████████████████████▌      | 4915/5928 [00:06<00:01, 782.75it/s]\u001b[A\n",
      " 84%|████████████████████████████████      | 4995/5928 [00:06<00:01, 787.51it/s]\u001b[A\n",
      " 86%|████████████████████████████████▌     | 5076/5928 [00:07<00:01, 791.96it/s]\u001b[A\n",
      " 87%|█████████████████████████████████     | 5156/5928 [00:07<00:00, 774.23it/s]\u001b[A\n",
      " 88%|█████████████████████████████████▌    | 5234/5928 [00:07<00:00, 762.22it/s]\u001b[A\n",
      " 90%|██████████████████████████████████    | 5311/5928 [00:07<00:00, 749.55it/s]\u001b[A\n",
      " 91%|██████████████████████████████████▌   | 5387/5928 [00:07<00:00, 741.56it/s]\u001b[A\n",
      " 92%|███████████████████████████████████   | 5462/5928 [00:07<00:00, 730.44it/s]\u001b[A\n",
      " 93%|███████████████████████████████████▍  | 5537/5928 [00:07<00:00, 733.65it/s]\u001b[A\n",
      " 95%|███████████████████████████████████▉  | 5612/5928 [00:07<00:00, 735.78it/s]\u001b[A\n",
      " 96%|████████████████████████████████████▍ | 5686/5928 [00:07<00:00, 734.42it/s]\u001b[A\n",
      " 97%|████████████████████████████████████▉ | 5760/5928 [00:07<00:00, 729.59it/s]\u001b[A\n",
      " 98%|█████████████████████████████████████▍| 5833/5928 [00:08<00:00, 724.35it/s]\u001b[A\n",
      "100%|██████████████████████████████████████| 5928/5928 [00:08<00:00, 721.14it/s]\u001b[A\n",
      "02/07/2024 17:09:05 - INFO - utils_qa - Saving predictions to ./tmp/result/eval_predictions.json.\n",
      "02/07/2024 17:09:05 - INFO - utils_qa - Saving nbest_preds to ./tmp/result/eval_nbest_predictions.json.\n",
      "100%|█████████████████████████████████████████| 757/757 [00:26<00:00, 28.56it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        2.0\n",
      "  eval_exact_match        =    71.8117\n",
      "  eval_f1                 =    81.7538\n",
      "  eval_runtime            = 0:00:13.60\n",
      "  eval_samples            =       6056\n",
      "  eval_samples_per_second =    444.966\n",
      "  eval_steps_per_second   =     55.621\n",
      "[INFO|modelcard.py:452] 2024-02-07 17:09:07,015 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}}\n"
     ]
    }
   ],
   "source": [
    "# Task: run the script\n",
    "!python transformers/examples/pytorch/question-answering/run_qa.py \\\n",
    "  --model_name_or_path $MODEL_NAME \\\n",
    "  --train_file $TRAIN_FILE_NAME \\\n",
    "  --validation_file $EVAL_FILE_NAME \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --per_device_train_batch_size 12 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --max_seq_length 384 \\\n",
    "  --doc_stride 128 \\\n",
    "  --output_dir ./tmp/result/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qktc-Lr0PfKP",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71ef8820bc48bf968b510ebd2e180630",
     "grade": false,
     "grade_id": "cell-b013aff665a537c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**c) To make the model available for other researchers you need to deploy it to the [Hugging Face Model Hub](https://huggingface.co/models). Don't forget to upload your tokenizer alongside the model. Insert the link of your deployed model as an answer for this task.**  **(1p)**\n",
    "\n",
    "Hint: You will need to create an account on Hugging Face and create a [repository](https://huggingface.co/new). \n",
    "\n",
    "**IMPORTANT:** Make the repository **public**!!! Save your model name in the variable `your_model_name`. You'll find the model name on top of your model repository on Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "id": "tAc_nJFPPfKP",
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8caffe908043947491d28409ce62e9ba",
     "grade": false,
     "grade_id": "cell-be478dceac7a7c92",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at tmp/result/checkpoint-14500 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 51.0M/51.0M [00:36<00:00, 1.41MB/s]\n"
     ]
    }
   ],
   "source": [
    "# insert the name of your shared model\n",
    "your_model_name = 'jahknem/xtremedistil-l6-h256-uncased' # The name from Hugging Face: `UserName/RepoName`\n",
    "checkpoint = 'tmp/result/checkpoint-14500'  # Example checkpoint\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model.push_to_hub(your_model_name)\n",
    "tokenizer.push_to_hub(your_model_name)\n",
    "\n",
    "link = f'https://huggingface.co/{your_model_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QDJv9Rx8PfKP",
    "jupyter": {
     "outputs_hidden": true
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c9ab3b44cf99e204f7e32d5846022391",
     "grade": true,
     "grade_id": "cell-bf02b5cbe1d35c61",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tests - Don't modify\n",
    "assert re.search(\"huggingface.co\", link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kOPpJV3PPfKQ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ad48e0fda565e5992022789a66a20cd",
     "grade": false,
     "grade_id": "cell-56ba9e9b66a212de",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Task 2: Analyze the model - 5 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6IBqmrfDPfKQ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73a758b8cc9523b18e98ea22044e7143",
     "grade": false,
     "grade_id": "cell-55fc7071e7839462",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "In this task, you will analyze models with regard to different questions. You should state your findings clearly and traceable including examples. You are allowed to include screenshots in your answers to make your answers understandable. This task is independent of task 1. Please select one of the follwing models: `DROP BERT Adapter`, `Xtremedistil-L6-H256-Uncased-NaturalQuestionsShort`, `SQuAD 2.0 BERT Adapter`. They should be available on the SQuARE website in the [*QA Hub*](https://square.ukp-lab.de/qa_hub).\n",
    "\n",
    "Hint: In general it's not expected that you write full essays to answer the questions. Short and clear answers are preferred and sufficient to achieve full points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "le_tUTTIPfKQ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebbd4e86377c655a85ab72ebbe151211",
     "grade": false,
     "grade_id": "cell-d8f0d9a5f13272cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**a) Which type of questions is your selected model able to answer? How is this type different from at least one other type of questions you know from the lecture?\n",
    "Please state which model you choose.** **(1p)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "Ifqv3pguPfKQ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d30cbe728b44426a51f07dc6495bf30",
     "grade": true,
     "grade_id": "cell-5a7a2f5389710f38",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UkqniupVPfKQ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "042a0b8ec0fdc70fe33f170df61130b9",
     "grade": false,
     "grade_id": "cell-a59ea71a460055c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**b) Analyze the general behavior of the model. Therefore run the model with different questions. Which type of questions is the model able to answer, and which are not? Interpret your results and provide examples for both.** **(2p)**\n",
    "\n",
    "Hint: It may help to compare the selected model to other models on the UKP-SQuARE platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "lCD29o6APfKR",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "785fe5e3c6c20935925c646043a8fe28",
     "grade": true,
     "grade_id": "cell-75563ba3314fecee",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5fdYJGrrPfKR",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb4b9581035de016defbfad47cebbb87",
     "grade": false,
     "grade_id": "cell-489ce64c65970eb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**c) Now we want to formalize the results from above. Explain the concept behind in-domain and out-of-domain data. Analyze your results and categorize the found examples from b) respectively. Which performance would you expect if you run your selected model on a full in-domain and out-of-domain dataset based on the knowledge you gained so far?** **(2p)**\n",
    "\n",
    "Hint: This concept will be taught in the lecture QA II. It might also help to think about on what dataset the model was trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "I_EEIW3YPfKR",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21de815bdcfdccc9a59a4c8575f360fa",
     "grade": true,
     "grade_id": "cell-8fc722fbaac3a37b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "n1DK61eZPfKR",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "571d0e63de9df5fa63475270e57919d6",
     "grade": false,
     "grade_id": "cell-cb2f72c02ad62572",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Task 3: Explainability - 6 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UDvYC1X8PfKV",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1ec178e495964d6c462a62ecfe44f48",
     "grade": false,
     "grade_id": "cell-346db9e086d97246",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "In this task, you will learn about the explainability of transformer-based models. For this purpose, saliency maps are a common method to visualize predictions in Deep Learning inspired by research in computer vision. In the NLP domain,  saliency maps attribute weights to every input token to assess the importance of the model prediction. The UKP-SQuARE platform provides two different family methods: gradient-based and attention-based.\n",
    "For learning more about it, you can read the following paper: https://aclanthology.org/2022.aacl-demo.4.pdf. or the respective Medium article https://medium.com/@ukp-square/interpreting-saliency-maps-for-question-a-with-ukp-square-a6b2831d8431.\n",
    "Both are not required to achieve full points on the tasks but help to understand the topics.\n",
    "\n",
    "Hint: You are highly encouraged to include screenshots to make your results understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4b3L9Gr0PfKW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f1ae8c7ca46e42757994d88b6df3545",
     "grade": false,
     "grade_id": "cell-c578cac764f304e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**a) Give a short introduction about why explainability is important in Deep Learning research? Because this is an NLP class, provide one example from the domain of NLP.** **(2p)**\n",
    "\n",
    "Hint: For the example, it's sufficient to think logically, it's not needed to cite papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "17JdkBsEPfKW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f77d53ab8c22306151eef4bd69431157",
     "grade": true,
     "grade_id": "cell-da7422f87eb7a398",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4y16FpznPfKW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "210c54ae2c52cfa3ea9f526ec08baabe",
     "grade": false,
     "grade_id": "cell-d6f06aa0800a2cf1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**b) In this subtask we start to analyze the predictions from your selected model with the respective saliency maps. Find at least one method that explains the prediction from your selected model and describe and interpret your results.** **(2p)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "QkdpvAbkPfKW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc65797886bacac481bd42f1df6ea7ec",
     "grade": true,
     "grade_id": "cell-714a5945fcec1a2d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "GprqaKSnPfKW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73fc32f734a2bee79f2773433c78ce78",
     "grade": false,
     "grade_id": "cell-4f12a95473ce9045",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**c) Explain one method that doesn't explain the prediction from your model and interpret your results. Prove your result with at least one question.** **(2p)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "oOjCV91fPfKW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93a99295d7d15d71de1b3bb2d4fe662b",
     "grade": true,
     "grade_id": "cell-96d50e6502d716a5",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "t_SVw7kdPfKW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e380db55e3e42307eb7319ed88a312a3",
     "grade": false,
     "grade_id": "cell-6b9b4a8b15d99d07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Task 4: Attacking your model - 4 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "lZ4mwUktPfKX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9291b2ea123a4f4755038817f96133c4",
     "grade": false,
     "grade_id": "cell-7f27c7bc20160152",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "In this task, you will learn about attacking your selected model.\n",
    "Therefore the UKP-SQuARE interface makes it easy to apply different input modifications to find vulnerabilities in the model. You will explore different attacking methods.\n",
    "For learning more about the attacking methods, you can read the following paper: https://aclanthology.org/2022.aacl-demo.4.pdf. or the respective Medium article https://medium.com/@ukp-square/interpreting-adversarial-attacks-in-question-answering-with-ukp-square-5f1866ade13c.\n",
    "Both are not required to achieve full points on the tasks but help to understand the topics.\n",
    "\n",
    "Hint: You are highly encouraged to include screenshots to make your results understandable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kB9NauDKPfKX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1cba19a66efd887b736de32eab374b01",
     "grade": false,
     "grade_id": "cell-fa4a6aecbbf24153",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**a) Identify whether and how your selected model can be fooled/hacked with the Input Reduction method. Interpret your findings in your own words.** **(1p)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "9O8aBmGxPfKX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "322ff214a0a6c79e65e612ebb9a8145a",
     "grade": true,
     "grade_id": "cell-3a63f63edf85abc7",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9fnx2Z9QPfKX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4a6d02ccefc580dafbbed4bc5d67f02",
     "grade": false,
     "grade_id": "cell-ee290e3f7770c31b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**b) Identify whether and how your selected model can be fooled/hacked with the Sub-Span method. Interpret your findings in your own words.** **(1p)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "s_xrbthzPfKX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06bca055cc5c17ecdbcb1c43e33c6b60",
     "grade": true,
     "grade_id": "cell-8beb429ded55e08e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JQpbljdhPfKX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bcbd67eb5dd2bcadc01bef5251fde9d3",
     "grade": false,
     "grade_id": "cell-dca171c140217290",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**c) Identify whether and how your selected model can be fooled/hacked with the Top K method. Interpret your findings in your own words.** **(1p)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "8ZzBLVuSPfKX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b070641065b4e8a9067ada71cf299caa",
     "grade": true,
     "grade_id": "cell-e2cf22c30823a3d1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "kTOQ8a79PfKc",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0cd98e0d37613a498b11eaebf0e2fb04",
     "grade": false,
     "grade_id": "cell-287dc38868273c81",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**d) Explain why it's interesting for you as a researcher to know how your selected model is attackable?** **(1p)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "TDbAIgUPN7zv",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e250e2880d33145b6933f0d8f128f37",
     "grade": true,
     "grade_id": "cell-c9990cd201d54481",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6dvalMM2jTUg",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4284833abcf1d3d2f84989bce4fd41e",
     "grade": false,
     "grade_id": "cell-f7e5db7b97dbedb9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "**Submission:**\n",
    "\n",
    "Please upload your submission to Moodle before the next exercise session <font color=\"red\">(Feb 09th, 23:59)</font>!\n",
    "\n",
    "Submission format: `homework 6.zip`\n",
    "\n",
    "Your submission should contain your filled out Jupyter notebook (naming schema: `homework 6.ipynb`) and any auxiliar files that are necessary to run your code (e.g., the datasets provided by us).\n",
    "\n",
    "Each submission must be handed in only once per group."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
